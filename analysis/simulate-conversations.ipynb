{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv('../.env')\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "with open('../task/snippets/string-anagram.py') as file:\n",
    "    code = file.read()\n",
    "language = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical prompts to /web/api/src/lib/agent.ts\n",
    "control_chain = RunnablePassthrough.assign(\n",
    "    history=lambda inputs: [message for turn in inputs['history'] for message in [('user', turn['input']), ('ai', turn['response'])]]\n",
    ") | RunnablePassthrough.assign(\n",
    "    response=ChatPromptTemplate.from_messages([\n",
    "    ('system', '''You are a helpful assistant. The user is given a code snippet which they have to understand. Note that the user did not write the code or provide the code snippet themselves.\n",
    "        \n",
    "Code: \"\"\"{language}\n",
    "{code}\n",
    "\"\"\"\n",
    "\n",
    "Produce an appropriate response to the user's input.'''),\n",
    "        MessagesPlaceholder('history'),\n",
    "        ('user', '{input}'),\n",
    "    ]) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "tom_chain = RunnablePassthrough.assign(\n",
    "    history=lambda inputs: [message for turn in inputs['history'] for message in [('user', turn['input']), ('ai', turn['response'])]]\n",
    ") | RunnablePassthrough.assign(\n",
    "    questions=ChatPromptTemplate.from_messages([\n",
    "        ('system', '''You are a helpful assistant. The user is given a code snippet which they have to understand. Note that the user did not write the code or provide the code snippet themselves.\n",
    "            \n",
    "Code: \"\"\"{language}\n",
    "{code}\n",
    "\"\"\"'''),\n",
    "        MessagesPlaceholder('history'),\n",
    "        ('user', '{input}'),\n",
    "        ('user', '''Identify what aspects of the user's mental state are directly relevant to producing a response to the user's input. Phrase these aspects as open-ended questions. The questions should be different enough from each other to each provide valuable insights. Make sure the questions are about the user's mental state and are not leading.''')\n",
    "    ]) | model | StrOutputParser()\n",
    ") | RunnablePassthrough.assign(\n",
    "    mental_state=ChatPromptTemplate.from_messages([\n",
    "        ('system', '''You are a helpful assistant. The user is given a code snippet which they have to understand. Note that the user did not write the code or provide the code snippet themselves.\n",
    "            \n",
    "Code: \"\"\"{language}\n",
    "{code}\n",
    "\"\"\"'''),\n",
    "        MessagesPlaceholder('history'),\n",
    "        ('user', '{input}'),\n",
    "        ('user', '''Take the perspective of the user. Answer the following questions about the user's mental state, and explain how this mental state has led to the user's messages. Note that the user did not write the code or provide the code snippet themselves. Therefore, you cannot base your answers on the code snippet or on any code the user copies and pastes, only on the conversation history. Phrase the answers as independent statements, not as responses to the questions.\n",
    "\n",
    "Questions: \"\"\"\n",
    "{questions}\n",
    "\"\"\".''')\n",
    "    ]) | model | StrOutputParser()\n",
    ") | RunnablePassthrough.assign(\n",
    "    response=ChatPromptTemplate.from_messages([\n",
    "        ('system', '''You are a helpful assistant. The user is given a code snippet which they have to understand. Note that the user did not write the code or provide the code snippet themselves.\n",
    "            \n",
    "Code: \"\"\"{language}\n",
    "{code}\n",
    "\"\"\"\n",
    "\n",
    "Produce an appropriate response to the user's input.'''),\n",
    "        MessagesPlaceholder('history'),\n",
    "        ('user', '{input}'),\n",
    "        ('user', '''You have identified the mental state of the user. Use this mental state to produce an appropriate response. Do not mention how the response is based on the user's mental state.\n",
    "                \n",
    "Mental state: \"\"\"\n",
    "{mental_state}\n",
    "\"\"\"''')\n",
    "    ]) | model | StrOutputParser()\n",
    ") | (lambda output: {\n",
    "    'questions': output['questions'],\n",
    "    'mental_state': output['mental_state'],\n",
    "    'response': output['response']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "user_simulation_chain = RunnablePassthrough.assign(\n",
    "    history=lambda inputs: [message for turn in inputs['history'] for message in [('user', turn['input']), ('ai', turn['response'])]]\n",
    ") | ChatPromptTemplate.from_messages([\n",
    "        ('system', '''You are a helpful assistant. The user is given a code snippet which they have to understand. Note that the user did not write the code or provide the code snippet themselves.\n",
    "            \n",
    "Code: \"\"\"{language}\n",
    "{code}\n",
    "\"\"\"'''),\n",
    "    MessagesPlaceholder('history'),\n",
    "    ('user', '''Produce a new question that the user might send to the assistant, by taking their perspective. Your programming experience level is '{experience_level}', but do not explicitly mention this.''')\n",
    "]) | ChatOpenAI(model='gpt-4', temperature=0.7) | StrOutputParser()\n",
    "\n",
    "def generate_conversation(filename, experience_level, code, language, n=5):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as file:\n",
    "            history = json.load(file)\n",
    "    else:\n",
    "        history = []\n",
    "\n",
    "    while len(history) < n:\n",
    "        input = user_simulation_chain.invoke({\n",
    "            'experience_level': experience_level,\n",
    "            'code': code,\n",
    "            'language': language,\n",
    "            'history': history,\n",
    "        })\n",
    "        \n",
    "        control = control_chain.invoke({\n",
    "            'code': code,\n",
    "            'language': language,\n",
    "            'history': history,\n",
    "            'input': input\n",
    "        })\n",
    "\n",
    "        tom = tom_chain.invoke({\n",
    "            'code': code,\n",
    "            'language': language,\n",
    "            'history': history,\n",
    "            'input': input\n",
    "        })\n",
    "\n",
    "        history.append({'input': input, 'response': control['response'], 'tom': tom})\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(history, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_conversation('data/simulated-conversation-advanced.json', 'Quite advanced', code, language, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_conversation('data/simulated-conversation-novice.json', 'Absolute beginner', code, language, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in ['advanced', 'novice']:\n",
    "    with open(f'data/simulated-conversation-{user}.json') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    mental_states_latex = ''\n",
    "    conversation_latex = ''\n",
    "\n",
    "    for turn in data:\n",
    "        mental_states_latex += '\\\\conversationmessageright{' + turn['input'] + '}\\n'\n",
    "        mental_states_latex += '\\\\conversationmessageleft{\\\\textbf{Inferred mental state:}\\n\\n' + turn['tom']['mental_state'] + '}\\n'\n",
    "\n",
    "        conversation_latex += '\\\\conversationmessageright{' + turn['input'] + '}\\n'\n",
    "        conversation_latex += '\\\\conversationmessageleft{\\\\textbf{Response control:}\\n\\n' + turn['response'].replace('\\n', '\\n\\n') + '}\\n'\n",
    "        conversation_latex += '\\\\conversationmessageleft{\\\\textbf{Response \\\\approach{}:}\\n\\n' + turn['tom']['response'].replace('\\n', '\\n\\n') + '}\\n'\n",
    "\n",
    "    with open(f'figures/simulated_conversation_mental_states_{user}.tex', 'w') as file:\n",
    "        file.write(mental_states_latex)\n",
    "    \n",
    "    with open(f'figures/simulated_conversation_{user}.tex', 'w') as file:\n",
    "        file.write(conversation_latex)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
